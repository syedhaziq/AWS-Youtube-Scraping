{
	"jobConfig": {
		"name": "transformation-haziq",
		"description": "",
		"role": [role],
		"command": "glueetl",
		"version": "4.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": "5",
		"maxCapacity": 5,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "transformation-haziq.py",
		"scriptLocation": [script location],
		"language": "python-3",
		"spark": false,
		"jobParameters": [],
		"tags": [],
		"jobMode": "NOTEBOOK_MODE",
		"createdOn": "2024-10-04T15:50:27.192Z",
		"developerMode": false,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-575108943635-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": false,
		"pythonShellPrebuiltLibraryOption": "analytics",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"bookmark": "",
		"metrics": "",
		"observabilityMetrics": "",
		"logging": "",
		"sparkPath": "",
		"serverEncryption": false,
		"pythonPath": "",
		"dependentPath": "",
		"referencedPath": "",
		"etlAutoScaling": false,
		"etlAutoTuningJobRules": "",
		"pythonVersion": ""
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n## Loading Views Dataframe\n\nviews_df = spark.read.csv('s3://s3-reddit-storage-haziq/raw/view_stats.csv', header=True, inferSchema= True)\n\nviews_df.show()\n### Loading Top Videos Dataframe ###\ntop_videos_df = spark.read.csv('s3://s3-reddit-storage-haziq/raw/top_videos_stats.csv', header=True, inferSchema= True)\n\ntop_videos_df.show()\nfrom pyspark.sql.functions import avg, lit\n\n#### Calculating Basic Stats ##\n\navg_views = top_videos_df.select(avg('viewcount')).first()[0]\navg_like = top_videos_df.select(avg('likecount')).first()[0]\navg_comment = top_videos_df.select(avg('commentcount')).first()[0]\n### Merging above generated stats in Views Dataframe ######\nviews_df_v2 = views_df.withColumn('avg_views',lit(avg_views)).withColumn('avg_like',lit(avg_like)).withColumn('avg_comment',lit(avg_comment))\nviews_df_v2.show()\n### Writing dataframes back into s3 bucket under the transformed folder ######\nviews_df_v2.repartition(1).write.mode('overwrite').option('header','true').csv('s3://s3-reddit-storage-haziq/transform/views_df_v2.csv')\ntop_videos_df.repartition(1).write.mode('overwrite').option('header','true').csv('s3://s3-reddit-storage-haziq/transform/top_videos_df.csv')\njob.commit()",
	"notebook": {
		"metadata": {
			"kernelspec": {
				"name": "glue_pyspark",
				"display_name": "Glue PySpark",
				"language": "python"
			},
			"language_info": {
				"name": "Python_Glue_Session",
				"mimetype": "text/x-python",
				"codemirror_mode": {
					"name": "python",
					"version": 3
				},
				"pygments_lexer": "python3",
				"file_extension": ".py"
			}
		},
		"nbformat_minor": 4,
		"nbformat": 4,
		"cells": [
			{
				"cell_type": "markdown",
				"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
				"metadata": {
					"editable": true,
					"trusted": true
				}
			},
			{
				"cell_type": "markdown",
				"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
				"metadata": {
					"editable": true,
					"trusted": true
				}
			},
			{
				"cell_type": "code",
				"source": "%help",
				"metadata": {
					"trusted": true,
					"editable": true
				},
				"execution_count": null,
				"outputs": []
			},
			{
				"cell_type": "markdown",
				"source": "####  Run this cell to set up and start your interactive session.\n",
				"metadata": {
					"editable": true,
					"trusted": true
				}
			},
			{
				"cell_type": "code",
				"source": "%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
				"metadata": {
					"trusted": true,
					"editable": true
				},
				"execution_count": 1,
				"outputs": [
					{
						"name": "stdout",
						"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: a5d46963-f983-4de2-b161-10ce4aa819ba\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session a5d46963-f983-4de2-b161-10ce4aa819ba to get into ready status...\nSession a5d46963-f983-4de2-b161-10ce4aa819ba has been created.\n\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "markdown",
				"source": "#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n",
				"metadata": {
					"editable": true,
					"trusted": true
				}
			},
			{
				"cell_type": "code",
				"source": "## Loading Views Dataframe\n\nviews_df = spark.read.csv('s3://s3-reddit-storage-haziq/raw/view_stats.csv', header=True, inferSchema= True)\n\nviews_df.show()",
				"metadata": {
					"trusted": true,
					"editable": true
				},
				"execution_count": 3,
				"outputs": [
					{
						"name": "stdout",
						"text": "+-----------+---------------+----------+\n|  viewCount|subscriberCount|videoCount|\n+-----------+---------------+----------+\n|60065943543|      318000000|       819|\n+-----------+---------------+----------+\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "### Loading Top Videos Dataframe ###\ntop_videos_df = spark.read.csv('s3://s3-reddit-storage-haziq/raw/top_videos_stats.csv', header=True, inferSchema= True)\n\ntop_videos_df.show()",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 4,
				"outputs": [
					{
						"name": "stdout",
						"text": "+--------------------+---------+---------+------------+------------+\n|               title|viewcount|likecount|dislikecount|commentcount|\n+--------------------+---------+---------+------------+------------+\n|$10,000 Every Day...|309685632|  6355058|           0|       82490|\n|Survive 100 Days ...|291172692|  6155474|           0|       92943|\n|Face Your Biggest...|266045034|  5927280|           0|      227647|\n|50 YouTubers Figh...|252615759|  8704429|           0|      356217|\n|I Saved 100 Dogs ...|234697816|  5844219|           0|      102710|\n|7 Days Stranded O...|233755752|  6249060|           0|      144608|\n|$1 vs $250,000,00...|228211841|  5339393|           0|      102890|\n|Ages 1 - 100 Deci...|224382329|  5707556|           0|      213799|\n|I Survived 7 Days...|217781022|  5979000|           0|      202229|\n|Worldâ€™s Deadliest...|216055208|  5997486|           0|      152190|\n|$1 vs $10,000,000...|204593796|  4421095|           0|       63226|\n|$10,000 Every Day...|200914683|  7468537|           0|      335496|\n|I Spent 7 Days Bu...|196451698|  6717194|           0|      110047|\n|Survive 100 Days ...|186460762|  5289849|           0|       87184|\n|I Spent 7 Days In...|183367486|  4870003|           0|       80734|\n|Protect The Lambo...|167076704|  4880921|           0|      126223|\n|Protect $500,000 ...|147785270|  4523081|           0|       72532|\n|I Built 100 House...|132360311|  7096709|           0|      291841|\n|Protect The Yacht...|126804565|  3931327|           0|       84976|\n|In 10 Minutes Thi...|124728203|  3966793|           0|       82711|\n+--------------------+---------+---------+------------+------------+\nonly showing top 20 rows\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "from pyspark.sql.functions import avg, lit\n\n#### Calculating Basic Stats ##\n\navg_views = top_videos_df.select(avg('viewcount')).first()[0]\navg_like = top_videos_df.select(avg('likecount')).first()[0]\navg_comment = top_videos_df.select(avg('commentcount')).first()[0]",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 16,
				"outputs": [
					{
						"name": "stdout",
						"text": "\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "markdown",
				"source": "#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n",
				"metadata": {
					"editable": true,
					"trusted": true
				}
			},
			{
				"cell_type": "code",
				"source": "### Merging above generated stats in Views Dataframe ######\nviews_df_v2 = views_df.withColumn('avg_views',lit(avg_views)).withColumn('avg_like',lit(avg_like)).withColumn('avg_comment',lit(avg_comment))",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 17,
				"outputs": [
					{
						"name": "stdout",
						"text": "DataFrame[viewCount: bigint, subscriberCount: int, videoCount: int, avg_views: double, avg_like: double, avg_comment: double]\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "views_df_v2.show()",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 18,
				"outputs": [
					{
						"name": "stdout",
						"text": "+-----------+---------------+----------+--------------------+------------------+------------------+\n|  viewCount|subscriberCount|videoCount|           avg_views|          avg_like|       avg_comment|\n+-----------+---------------+----------+--------------------+------------------+------------------+\n|60065943543|      318000000|       819|1.9340069439130434E8|5456818.0869565215|138391.95652173914|\n+-----------+---------------+----------+--------------------+------------------+------------------+\n",
						"output_type": "stream"
					}
				]
			},
			{
				"cell_type": "code",
				"source": "### Writing dataframes back into s3 bucket under the transformed folder ######\nviews_df_v2.repartition(1).write.mode('overwrite').option('header','true').csv('s3://s3-reddit-storage-haziq/transform/views_df_v2.csv')\ntop_videos_df.repartition(1).write.mode('overwrite').option('header','true').csv('s3://s3-reddit-storage-haziq/transform/top_videos_df.csv')",
				"metadata": {
					"trusted": true,
					"tags": []
				},
				"execution_count": 22,
				"outputs": [
					{
						"name": "stdout",
						"text": "\n",
						"output_type": "stream"
					}
				]
			}
		]
	}
}